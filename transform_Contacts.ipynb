{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c48ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the EXTRACT, TRANSFORM, LOAD directories\n",
    "# in a default environment setup, these would be relative to this .ipynb (or .py if this notebook has been converted) \n",
    "\n",
    "dir_extract=\"./01.DataExtract/\"\n",
    "dir_transform=\"./02.DataTransform/\"\n",
    "dir_load=\"./03.DataLoad/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ad8ac",
   "metadata": {},
   "source": [
    "### Test File = \"test_contacts.csv\"\n",
    "\n",
    "- mark - multiple (4) identical records\n",
    "- michael - multiple (3) indentical records\n",
    "- kevin - 3 unique records; 2 with same Contact ID but different email2 values, 1 with unique Contact ID but rest of data identical to one of the other records.\n",
    "- maggie - 2 records, 2nd record have different Primary Email and email2 values\n",
    "- herb - 2 records, 2nd record has a blank Primary Email field\n",
    "- sydney - 2 records, 2nd record has blank Contact ID\n",
    "- jeff - 1 unique record with blank Contact ID\n",
    "\n",
    "### KR file = \"KR_Contacts_Names_Emails_00000_80000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input filenames\n",
    "\n",
    "filein_allRecords = \"KR_Contacts_Names_Emails_00000_80000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9140fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output filenames\n",
    "\n",
    "# initial sets of duplicate records\n",
    "fileout_duplicates = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_duplicates.csv\")\n",
    "print(fileout_duplicates)\n",
    "\n",
    "# working file with initial sets of duplicate records removed\n",
    "fileout_workingStep01 = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_workingStep01.csv\")\n",
    "print(fileout_workingStep01)\n",
    "\n",
    "# working file with de-duped records added back in\n",
    "fileout_workingStep02 = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_workingStep02.csv\")\n",
    "print(fileout_workingStep02)\n",
    "\n",
    "# records for staff manual cleanup of CiviCRM - records with Duplicate Contact IDs\n",
    "fileout_manualCleanupDupeOnContactID = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_manualCleanupDupeOnContactID.csv\")\n",
    "print(fileout_manualCleanupDupeOnContactID)\n",
    "\n",
    "# records for staff manual cleanup of CiviCRM - records with Duplicate Primary Emails (different Contact IDs)\n",
    "fileout_manualCleanupDupeOnEmail = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_manualCleanupDupeOnEmail.csv\")\n",
    "print(fileout_manualCleanupDupeOnEmail)\n",
    "\n",
    "# records for staff manual cleanup of CiviCRM - records with blank Contact IDs - something has gone horribly wrong\n",
    "fileout_manualCleanupBlankContactID = (\n",
    "    \"out_\" +\n",
    "    filein_allRecords.replace(\".csv\",\"\").replace(\"_\",\"\") +\n",
    "    \"_manualCleanupBlankContactID.csv\")\n",
    "print(fileout_manualCleanupBlankContactID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c545f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the initial extract file into dataframe\n",
    "df_extract = pd.read_csv(dir_extract + filein_allRecords)\n",
    "\n",
    "# create a working dataframe\n",
    "df_working = df_extract\n",
    "\n",
    "print(df_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect duplicates for further processing\n",
    "mask_duplicateRows = df_working.duplicated(keep=False)\n",
    "\n",
    "print(f\"\\n{mask_duplicateRows}\")\n",
    "\n",
    "df_duplicates = df_working[mask_duplicateRows].sort_values(by='Contact ID')\n",
    "print(f\"\\n{df_duplicates}\")\n",
    "\n",
    "# Write duplicates to a file\n",
    "df_duplicates.to_csv(dir_transform + fileout_duplicates, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536965",
   "metadata": {},
   "source": [
    "### !!! in KR's file, this produces 421 rows (odd!)\n",
    "this means there are sets where there an odd number of duplicate records\n",
    "in this case, example is\n",
    "\n",
    "| Contact | No. | Name |\n",
    "| -- | -- | -- |\n",
    "| 2063 | 3 | Julie Gates\n",
    "| 6340 | 3 | Cliff Mewdell\n",
    "| 9501 | 3 | Josh Shook\n",
    "| 12610 | 3 | Sage Walker\n",
    "| 13141 | 3 | Michael Rehak\n",
    "| 18413 | 3 | Damian Fox\n",
    "| 19423 | 4 | marco tejada\n",
    "| 21086 | 4 | Lindsay Hacker\n",
    "| 28647 | 4 | Stephen Moore\n",
    "| 31793 | 3 | Hyedie Hashimoto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed606061",
   "metadata": {},
   "source": [
    "# SCRATCH\n",
    "\n",
    "# Count the number of occurrences of each value in the 'Contact ID' column\n",
    "counts = df_duplicates.groupby('Contact ID')['Contact ID'].transform('count')\n",
    "\n",
    "# Create a mask for rows where 'Contact ID' occurs exactly 3 times\n",
    "mask = counts == 3\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "df_filtered = df_duplicates[mask]\n",
    "\n",
    "print(df_filtered)\n",
    "\n",
    "\n",
    "# Create a mask for rows where 'Contact ID' occurs exactly 3 times\n",
    "mask = counts == 3\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "df_filtered = df_duplicates[mask]\n",
    "\n",
    "print(df_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sets of records that are duplicate, drop them\n",
    "df_working = df_working[~mask_duplicateRows]\n",
    "print(df_working)\n",
    "\n",
    "# Write to a file\n",
    "df_working.to_csv(dir_transform + fileout_workingStep01, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee45b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the duplicates, removing all but the first row\n",
    "df_duplicates_deduped_onFirst = df_duplicates.drop_duplicates(keep='first')\n",
    "print(df_duplicates_deduped_onFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH\n",
    "df_duplicates_deduped_onFirst.to_csv(dir_transform + \"XXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these can then be added back to the working dataframe because they are now unique records\n",
    "df_working = pd.concat([df_working, df_duplicates_deduped_onFirst])\n",
    "df_working.sort_values(by='Contact ID', inplace=True)\n",
    "print(df_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a99404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "df_working.to_csv(dir_transform + fileout_workingStep02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to deal with issue where there exist sets of identical Contact ID where other fields are different e.g., Primary Email.\n",
    "# need to make sure we don't include sets where Contact ID is null/NaN\n",
    "# because that could end up including different people whose records simply have null/NaN Contact ID in error\n",
    "# those will be caught later\n",
    "\n",
    "mask_manualCleanupDupeOnContactID = df_working['Contact ID'].duplicated(keep=False) & df_working['Contact ID'].notnull()\n",
    "df_manualCleanupDupeOnContactID = df_working[mask_manualCleanupDupeOnContactID]\n",
    "df_manualCleanupDupeOnContactID = df_manualCleanupDupeOnContactID.sort_values(by='Contact ID')\n",
    "print(df_manualCleanupDupeOnContactID)\n",
    "\n",
    "# Provide this file to staff for cleanup in the Civi source.\n",
    "df_manualCleanupDupeOnContactID.to_csv(dir_transform + fileout_manualCleanupDupeOnContactID, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_working = df_working[~mask_manualCleanupDupeOnContactID]\n",
    "df_working.sort_values(by='Contact ID', inplace=True)\n",
    "print(df_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to deal with records where primary email address (Primary Email) are duplicate but Contact ID is different\n",
    "# is this also where we capture Contact ID is NaN/null?\n",
    "\n",
    "mask_manualCleanupDupeOnEmail = df_working.duplicated(subset='Primary Email', keep=False)\n",
    "df_manualCleanupDupeOnEmail = df_working[mask_manualCleanupDupeOnEmail]\n",
    "df_manualCleanupDupeOnEmail = df_manualCleanupDupeOnEmail.sort_values(by='Primary Email')\n",
    "print(df_manualCleanupDupeOnEmail)\n",
    "\n",
    "# Provide this file to staff for cleanup in the Civi source.\n",
    "df_manualCleanupDupeOnEmail.to_csv(dir_transform + fileout_manualCleanupDupeOnEmail, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca480193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_working.drop_duplicates(subset=\"Primary Email\",keep=False, inplace=True)\n",
    "df_working = df_working[~mask_manualCleanupDupeOnEmail]\n",
    "df_working.sort_values(by=['Contact ID'], inplace=True)\n",
    "print(df_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for record with blank/NaN Contact IDs that have not already been captured in searches for duplicates\n",
    "mask_manualCleanupBlankContactID = df_working['Contact ID'].isna()\n",
    "df_manualCleanupBlankContactID = df_working[mask_manualCleanupBlankContactID]\n",
    "df_manualCleanupBlankContactID = df_manualCleanupBlankContactID.sort_values(by='Primary Email')\n",
    "print(df_manualCleanupBlankContactID)\n",
    "\n",
    "# Provide this file to staff for cleanup in the Civi source.\n",
    "df_manualCleanupBlankContactID.to_csv(dir_transform + fileout_manualCleanupBlankContactID, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_working = df_working[~mask_manualCleanupBlankContactID]\n",
    "df_working.sort_values(by=['Contact ID'], inplace=True)\n",
    "print(df_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c412505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
